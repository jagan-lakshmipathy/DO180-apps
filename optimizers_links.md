# Important links for Optimizers:

1. A Good Survey of Optimizers: \ https://ruder.io/optimizing-gradient-descent/

2. RMSProp Optimizer - The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. The difference between RMSprop and gradient descent is on how the gradients are calculated. \
https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b

3. Adam Optimizer: Adam is an adaptive learning rate optimization algorithm thatâ€™s been designed specifically for training deep neural networks. The following is a good link to learn about the Adam:

https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c